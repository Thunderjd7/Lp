Dataset: mall-customers This dataset gives the data of Income and money spent by the customers visiting a Shopping Mall. The data set contains Customer ID, Gender, Age, Annual Income, Spending Score. Therefore, as a mall owner you need to find the group of people who are the profitable customers for the mall owner. Apply at least two clustering algorithms (based on Spending Score) to find the group of customers. a. Apply Data pre-processing (Label Encoding , Data Transformationâ€¦.) techniques if necessary. b. Perform data-preparation( Train-Test Split) c. Apply Machine Learning Algorithm d. Evaluate Model.




# Step 1: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.model_selection import train_test_split
from scipy.cluster.hierarchy import dendrogram, linkage

# Step 2: Load Dataset
data = pd.read_csv("Mall_Customers.csv")
print(data.head())

# Step 3: Data Preprocessing
# Check for missing values
print(data.isnull().sum())

# Label Encoding for Gender
le = LabelEncoder()
data['Gender'] = le.fit_transform(data['Gender'])  # Male=1, Female=0

# Select Features
X = data[['Annual Income (k$)', 'Spending Score (1-100)']]

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Data Preparation (Train-Test Split)
X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)

# Step 5a: K-Means Clustering
# Find optimal number of clusters using Elbow Method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_train)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia, marker='o')
plt.title("Elbow Method - Optimal K")
plt.xlabel("Number of clusters")
plt.ylabel("Inertia")
plt.show()

# Choose K = 5 (based on elbow)
kmeans = KMeans(n_clusters=5, random_state=42)
y_kmeans = kmeans.fit_predict(X_train)

# Visualize K-Means Clusters
plt.figure(figsize=(8,6))
plt.scatter(X_train[y_kmeans==0,0], X_train[y_kmeans==0,1], label='Cluster 1')
plt.scatter(X_train[y_kmeans==1,0], X_train[y_kmeans==1,1], label='Cluster 2')
plt.scatter(X_train[y_kmeans==2,0], X_train[y_kmeans==2,1], label='Cluster 3')
plt.scatter(X_train[y_kmeans==3,0], X_train[y_kmeans==3,1], label='Cluster 4')
plt.scatter(X_train[y_kmeans==4,0], X_train[y_kmeans==4,1], label='Cluster 5')
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=200, c='black', marker='X', label='Centroids')
plt.title("Customer Segments using K-Means")
plt.xlabel("Annual Income (scaled)")
plt.ylabel("Spending Score (scaled)")
plt.legend()
plt.show()

# Step 5b: Hierarchical Clustering
linked = linkage(X_train, method='ward')

# Dendrogram
plt.figure(figsize=(10, 5))
dendrogram(linked)
plt.title("Dendrogram for Hierarchical Clustering")
plt.xlabel("Customers")
plt.ylabel("Euclidean Distances")
plt.show()

# Apply Agglomerative Clustering
hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X_train)

# Visualize Hierarchical Clusters
plt.figure(figsize=(8,6))
plt.scatter(X_train[y_hc==0,0], X_train[y_hc==0,1], label='Cluster 1')
plt.scatter(X_train[y_hc==1,0], X_train[y_hc==1,1], label='Cluster 2')
plt.scatter(X_train[y_hc==2,0], X_train[y_hc==2,1], label='Cluster 3')
plt.scatter(X_train[y_hc==3,0], X_train[y_hc==3,1], label='Cluster 4')
plt.scatter(X_train[y_hc==4,0], X_train[y_hc==4,1], label='Cluster 5')
plt.title("Customer Segments using Hierarchical Clustering")
plt.xlabel("Annual Income (scaled)")
plt.ylabel("Spending Score (scaled)")
plt.legend()
plt.show()

# Step 6: Model Evaluation (Silhouette Score)
from sklearn.metrics import silhouette_score

kmeans_score = silhouette_score(X_train, y_kmeans)
hc_score = silhouette_score(X_train, y_hc)

print("Silhouette Score for K-Means: ", kmeans_score)
print("Silhouette Score for Hierarchical: ", hc_score)
